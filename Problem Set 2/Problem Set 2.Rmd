---
title: "Problem Set 2"
author: "Se Hyun Kim"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(haven)
library(Matching)
library(ebal)
library(lmtest)
library(modelsummary)
library(stargazer)
library(sandwich)
library(kableExtra)
ne <- read_dta("nsw_exper.dta")
npw <- read_dta("nsw_psid_withtreated.dta")
```

## Problem 1A
$\pi(X_i) \equiv Pr(D_i = 1 | X_i)$
The propensity score is the probability of getting the treatment given (conditional on) $X_i$ (which in this case is a covariate vector).

## Problem 1B
$\mathbb{E}[Y_i|D_i = 1,\pi(X_{i})] - \mathbb{E}[Y_i|D_i = 0,\pi(X_{i})]$ is the difference of expectations of the treated and control potential outcomes given a propensity score $\pi(X_{i})$. $\pi(X_{i})$ is the probability density function of the propensity score. The whole integral is weighted sum of total ATE, which is the expected ATE.  
The identification result holds under the assumptions of the conditional ignorability (treatment assignment D is independent of the potential outcomes $Y_{i}(1)$ (treated) and $Y_{i}(0)$ (untreated) given a propensity score $\pi(X_{i})$) and common support (every unit can be treated or not treated. not 100%).  
The assumption can be expressed in terms of $\pi(X_i)$ as follows  
1. Conditional Ignorability: ${Y_{i}(0),Y_{i}(1)}\perp D_i|\pi(X_i)$  
2. Common Support: $0 < Pr(D_i = 1|\pi(X_i)) < 1$

## Problem 2A
Under the assumption of conditional ignorability and common support, the ATT can be identified in the following way.  
$\tau_{ATT} = E[Y_i(1)-Y_i(0)|D_i=1]$  
Based on the law of iterated expectation, this equation can be reformulated as follows. $\tau_{ATT} = E[E[Y_i(1)-Y_i(0)|D_i=1]D_i=1]$ This in turn, can be reformulated using the definition of $\mathbb{E}$ as $\tau_{ATT} = \int E[Y_i(1)-Y_i(0)|D_i=1]f(x|D_i)dx$.  
$\tau_{ATT} = \int [E[Y_i|X_i = x, D_i= 1] - E[Y_i|X_i = x, D_i = 0]f(x|D_i = 1)dx$  
$\tau_{ATT} = E[\hat{\tau}|D_i = 1]$

## Problem 2B
One would pick matching over regression if one wants to avoid assuming certain forms. Under matching, the treatment effect is nonparametrically identified. However, regression assumes linearity. If units have constant treatment effects, regression estimator is unbiased.

## Problem 2C
Matching and regression will give the same estimates of the ATT if the control and treatment groups are balanced.

## Problem 3A
The Euclidean distance between observations $i$ and $j$ is $d(x_i,x_j) = \sqrt{(x_{i1}-x_{j1})^2 + ... + (x_{iP} - x_{jP})^2}$ where $P$ is the number of pre-treatment covariates.

## Problem 3B
```{r}
# If perseverance had a form, this is what it would look like.
X1 <- rnorm(500, 0, 1)
X2 <- rnorm(500, 0, 1)
X3 <- rnorm(500, 0, 1)
X4 <- rnorm(500, 0, 1)
X5 <- rnorm(500, 0, 1)
X6 <- rnorm(500, 0, 1)
X7 <- rnorm(500, 0, 1)
X8 <- rnorm(500, 0, 1)
X9 <- rnorm(500, 0, 1)
X10 <- rnorm(500, 0, 1)
X11 <- rnorm(500, 0, 1)
X12 <- rnorm(500, 0, 1)
X13 <- rnorm(500, 0, 1)
X14 <- rnorm(500, 0, 1)
X15 <- rnorm(500, 0, 1)
X16 <- rnorm(500, 0, 1)
X17 <- rnorm(500, 0, 1)
X18 <- rnorm(500, 0, 1)
X19 <- rnorm(500, 0, 1)
X20 <- rnorm(500, 0, 1)
X <- replicate(20, 0)
b3 <- cbind(X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20)
b3 <- rbind(b3, X) %>% 
  as_tibble()


dis_closest1 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2)
  if(dis_cov < dis_closest1){
    dis_closest1 <- dis_cov
  } else {
    dis_closest1 <- dis_closest1
  }
}

dis_closest2 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2)
  if(dis_cov < dis_closest2){
    dis_closest2 <- dis_cov
  } else {
    dis_closest2 <- dis_closest2
  }
}

dis_closest3 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2)
  if(dis_cov < dis_closest3){
    dis_closest3 <- dis_cov
  } else {
    dis_closest3 <- dis_closest3
  }
}

dis_closest4 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2)
  if(dis_cov < dis_closest4){
    dis_closest4 <- dis_cov
  } else {
    dis_closest4 <- dis_closest4
  }
}

dis_closest5 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2)
  if(dis_cov < dis_closest5){
    dis_closest5 <- dis_cov
  } else {
    dis_closest5 <- dis_closest5
  }
}

dis_closest6 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2)
  if(dis_cov < dis_closest6){
    dis_closest6 <- dis_cov
  } else {
    dis_closest6 <- dis_closest6
  }
}

dis_closest7 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2)
  if(dis_cov < dis_closest7){
    dis_closest7 <- dis_cov
  } else {
    dis_closest7 <- dis_closest7
  }
}

dis_closest8 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2)
  if(dis_cov < dis_closest8){
    dis_closest8 <- dis_cov
  } else {
    dis_closest8 <- dis_closest8
  }
}

dis_closest9 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2)
  if(dis_cov < dis_closest9){
    dis_closest9 <- dis_cov
  } else {
    dis_closest9 <- dis_closest9
  }
}

dis_closest10 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2)
  if(dis_cov < dis_closest10){
    dis_closest10 <- dis_cov
  } else {
    dis_closest10 <- dis_closest10
  }
}

dis_closest11 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2)
  if(dis_cov < dis_closest11){
    dis_closest11 <- dis_cov
  } else {
    dis_closest11 <- dis_closest11
  }
}

dis_closest12 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2)
  if(dis_cov < dis_closest12){
    dis_closest12 <- dis_cov
  } else {
    dis_closest12 <- dis_closest12
  }
}

dis_closest13 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2)
  if(dis_cov < dis_closest13){
    dis_closest13 <- dis_cov
  } else {
    dis_closest13 <- dis_closest13
  }
}

dis_closest14 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2)
  if(dis_cov < dis_closest14){
    dis_closest14 <- dis_cov
  } else {
    dis_closest14 <- dis_closest14
  }
}

dis_closest15 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2)
  if(dis_cov < dis_closest15){
    dis_closest15 <- dis_cov
  } else {
    dis_closest15 <- dis_closest15
  }
}

dis_closest16 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2 + b3[[i,16]]^2)
  if(dis_cov < dis_closest16){
    dis_closest16 <- dis_cov
  } else {
    dis_closest16 <- dis_closest16
  }
}

dis_closest17 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2 + b3[[i,16]]^2
                  + b3[[i,17]]^2)
  if(dis_cov < dis_closest17){
    dis_closest17 <- dis_cov
  } else {
    dis_closest17 <- dis_closest17
  }
}

dis_closest18 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2 + b3[[i,16]]^2
                  + b3[[i,17]]^2 + b3[[i,18]]^2)
  if(dis_cov < dis_closest18){
    dis_closest18 <- dis_cov
  } else {
    dis_closest18 <- dis_closest18
  }
}

dis_closest19 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2 + b3[[i,16]]^2
                  + b3[[i,17]]^2 + b3[[i,18]]^2 + b3[[i,19]]^2)
  if(dis_cov < dis_closest19){
    dis_closest19 <- dis_cov
  } else {
    dis_closest19 <- dis_closest19
  }
}

dis_closest20 <- 100000000000000
for (i in 1:500) {
  dis_cov <- sqrt(b3[[i,1]]^2 + b3[[i,2]]^2 + b3[[i,3]]^2 + b3[[i,4]]^2
                  + b3[[i,5]]^2 + b3[[i,6]]^2 + b3[[i,7]]^2 + b3[[i,8]]^2
                  + b3[[i,9]]^2 + b3[[i,10]]^2 + b3[[i,11]]^2 + b3[[i,12]]^2
                  + b3[[i,13]]^2 + b3[[i,14]]^2 + b3[[i,15]]^2 + b3[[i,16]]^2
                  + b3[[i,17]]^2 + b3[[i,18]]^2 + b3[[i,19]]^2 + b3[[i,20]]^2)
  if(dis_cov < dis_closest20){
    dis_closest20 <- dis_cov
  } else {
    dis_closest20 <- dis_closest20
  }
}

b3_a <- c(1:20)
b3_b <- c(dis_closest1, dis_closest2, dis_closest3, dis_closest4, 
          dis_closest5, dis_closest6, dis_closest7, dis_closest8,
          dis_closest9, dis_closest10, dis_closest11, dis_closest12,
          dis_closest13, dis_closest14, dis_closest15, dis_closest16,
          dis_closest17, dis_closest18, dis_closest19, dis_closest20) 
b3_c <- cbind(b3_a, b3_b)%>% 
  as_tibble()

b3_c %>% 
  ggplot(aes(x = b3_a, y = b3_b)) +
  geom_line(color = "#5499C7", 
            size = 0.7) +
  geom_point(alpha = 0.7,
             color = "#D98880", 
             size = 1) +  
  labs(title = "The Changes in the Distance Based on the Number of Covariates",
       x = "The Number of Covariates Included",
       y = "The Distance of the Nearest Obs from the New Obs") +
  theme_bw()
```

## Problem 3C
It demonstrates that as the number of covariates increase, the distance a covariate and the reference point increases. This demonstrates the curse of dimensionality. As the number of covariates increase (dimension increases), the data sparsity exponentially increases.

## Problem 4A
```{r, warning=FALSE}
ne_t <- filter(ne, nsw == 1)
ne_c <- filter(ne, nsw == 0)
nsw_effect4a <- mean(ne_t$re78) - mean(ne_c$re78)
nsw_sd4a <- sqrt(var(ne_t$re78)/length(ne_t$re78) + var(ne_c$re78)/length(ne_c$re78))
nsw_effect4a
nsw_sd4a

a4 <- lm(re78 ~ nsw + age + educ + hisp +black + married + re74 + u74, data = ne)
coeftest(a4, vcov = vcovHC(a4, type = "HC2")) 
# using the robust std. error for possible heterockedasticity
```
The difference between the coefficient values from the difference-in-means and the regression are similar (1794.343 vs. 1720.754) because the treatment was randomized. The small difference could be explained by (1) it is unlikely that everything will be 100% balanced and (2) certain confounders.


## Problem 4B
```{r}
npw_t <- filter(npw, nsw == 1)
npw_c <- filter(npw, nsw == 0)
nsw_effect4b <- mean(npw_t$re78) - mean(npw_c$re78)
nsw_sd4b <- sqrt(var(npw_t$re78)/length(npw_t$re78) + var(npw_c$re78)/length(npw_c$re78))
nsw_effect4b
nsw_sd4b

b4 <- lm(re78 ~ nsw + age + educ + hisp +black + married + re74 + u74, data = npw)
coeftest(b4, vcov = vcovHC(b4, type = "HC2")) # same reason as above
```
The coefficient values from the difference-in-means and the regression are different (-15204.78 vs. -1459.6). This shows that the treatment has not been randomized since it is from an observational data. Since the treatment is not randomized, we should match the covariates to make the treatment "as-if" random.

## Problem 4C
```{r}
mb_4c <- MatchBalance(nsw ~ age + educ + black + hisp + married + 
               re74 + u74 + re75 + u75, data = npw)
bt_4c <- baltest.collect(matchbal.out = mb_4c,
                                        var.names = colnames(npw)[-c(1,9,12)],
                                        after = FALSE)

bt_4c[,-8:-10] %>%
  round(3) %>%  
  kable(col.names = c("Mean Treatment","Mean Control","Std Dev","Std Dev Pooled","Var Ratio","T P-v","KS P-v"),
        caption = "Chekcing Covariate Balance",
        align = "c")
```
Every covariate, except `hisp`, seems to be an important factor in treatment. The differences between the Mean Treatment and Mean Control are significant for all other covariates (except `hisp`). The difference in means is not by chance because the low p-value shows that the null hypothesis can be rejected. The covariates are not balanced.

## Problem 4D
```{r}
# for experimental data
# The following two lines won't knit...
covar1 <- ne[,c(2,3,4,5,6,7,10)]
pscore.fmla1 <- as.formula(paste("nsw ~", paste(names(covar1), collapse = "+")))
pscore_model1 <- glm(pscore.fmla1, data = ne, family = binomial(link = logit))
pscore1 <- predict(pscore_model1, type = "response")
match.pscore1 <- Match(Tr = ne$nsw, X = pscore1, M = 1, estimand = "ATT")
MatchBalance(pscore.fmla1, data = npw, match.out = match.pscore1)

ggplot(data = ne, 
       aes(x = pscore1,
           group = as.factor(nsw),
           fill = as.factor(nsw))) +
  geom_histogram(alpha = .6) +
  labs(title = "The Distribution of Propensity Scores",
       x = "The Propensity Scores",
       y = "Count") +
  stat_bin(bins = 30) +
  theme_bw()
  

# for non-experimental data
covar <- npw[,c(2,3,4,5,6,7,10)]
pscore.fmla <- as.formula(paste("nsw ~", paste(names(covar), collapse = "+")))
pscore_model <- glm(pscore.fmla, data = npw, family = binomial(link = logit))
pscore <- predict(pscore_model, type = "response")
match.pscore <- Match(Tr = npw$nsw, X = pscore, M = 1, estimand = "ATT")
MatchBalance(pscore.fmla, data = npw, match.out = match.pscore)

ggplot(data = npw,
         aes(x = pscore,
             group = as.factor(nsw),
             fill = as.factor(nsw))) +
    geom_histogram(alpha = .6) +
  labs(title = "The Distribution of Propensity Scores",
       x = "The Propensity Scores",
       y = "Count") +
  stat_bin(bins = 30) +
  theme_bw()
```
The distribution of the propensity scores for the experimental data demonstrates that the treatment has been randomized. On the other hand, the distribution of the propensity scores for the non-experimental data demonstrates the contrary. They the propensity scores for the treated and the untreated are skewed to the opposite direction.

## Problem 4E
```{r}
m1 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 1, Weight = 2, BiasAdjust = TRUE)
summary(m1)

m2 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 4, Weight = 2, BiasAdjust = TRUE)
summary(m2)

m3 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 10, Weight = 2, BiasAdjust = TRUE)
summary(m3)

m4 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 1, Weight = 2)
summary(m4)

m5 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 4, Weight = 2)
summary(m5)

m6 <- Match(Y = npw$re78, Tr = npw$nsw, X = covar, M = 10, Weight = 2)
summary(m6)

```

## Problem 4F
```{r}
m4f <- Match(Y = npw$re78, Tr = npw$nsw, X = pscore, M = 1, Weight = 2, BiasAdjust = TRUE)
summary(m4f)
```

## Problem 4G
```{r}
g4 <- sum(npw$re78 * (npw$nsw - pscore) / (1 - pscore)) / length(npw_t$nsw) # from 4b filtered for 4b
g4
```
The ATT by weighting on the propensity score does not accord with the previous results. The problem is that PS weighting can be biased if the samples are small or there are multiple continuous variables. Bias must be adjusted. Since there are four continuous covariates in our model, PS weighting without bias adjustment will give a biased estimate.

## Problem 4H
The NSW program is effective. First, the results from the experimental data demonstrate its effectiveness. The histogram from 4D demonstrates that the treatment in the experimental data was successfully randomized. Second, our results from 4F, matching with propensity score, also demonstrates that the program is effective (if conditional ignorability and common support are assumed). The ATT with bias corrected matching estimators and `M = 4` gave a similar results but without the treatment being randomized, we can't be certain of its unbiasedness. Therefore, the use of propensity scores on 4F is better. By comparing results from 4F and 4G, we can see that the results for 4F have adjusted for the bias caused by matching.



















